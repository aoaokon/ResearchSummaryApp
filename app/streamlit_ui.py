# app/streamlit_ui.py
import streamlit as st
import pandas as pd
import tempfile
import sys
import os
import json
import requests
from datetime import datetime
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from utils.arxiv_search import search_arxiv
from utils.semantic_scholar_search import search_semantic_scholar
from utils.pdf_text_extractor import extract_text_from_pdf, save_text_to_file
from utils.summarizer import summarize_text
from utils.pdf_downloader import download_pdf
from utils.summarizer import save_summary_to_file
from utils.db_manager import init_db, insert_or_update_paper, update_paper_status, update_summary_to_db
from utils.db_manager import fetch_all_papers

init_db()  # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åˆæœŸåŒ–

st.set_page_config(page_title="è«–æ–‡æ¤œç´¢ï¼†PDFè¦ç´„", layout="wide")
st.title("ğŸ“š è«–æ–‡æ¤œç´¢ï¼†PDFè¦ç´„ã‚¢ãƒ—ãƒª")

# ---------------------------------------
# ğŸ” ã‚»ã‚¯ã‚·ãƒ§ãƒ³1ï¼šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼ˆarXiv + Semantic Scholarï¼‰
# ---------------------------------------
st.header("ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰è«–æ–‡ã‚’æ¤œç´¢")

keyword = st.text_input("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", placeholder="ä¾‹: Transformer, Climate Change")
max_results = st.slider("è«–æ–‡ã‚’æ¤œç´¢ã™ã‚‹ã‚½ãƒ¼ã‚¹ã¯arXivã¨semantic scholarã§ã™ã€‚ãã‚Œãã‚Œã®ã‚½ãƒ¼ã‚¹ã§å–å¾—ã™ã‚‹è«–æ–‡æ•°ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚ãã®ä¸­ã‹ã‚‰PDFãŒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã‚‹ã‚‚ã®ã®ã¿æŠ½å‡ºã—ã¾ã™ã€‚", 1, 10, 5)

all_papers = []

if keyword:
    st.success(f"æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼š{keyword}")

    with st.spinner("è«–æ–‡æ¤œç´¢ä¸­..."):
        arxiv_results = search_arxiv(keyword, max_results=max_results)
        try:
            semsch_results = search_semantic_scholar(keyword, limit=max_results)
        except requests.exceptions.HTTPError as e:
            st.warning(f"Semantic Scholar APIåˆ¶é™ã®ãŸã‚ã€Semantic Scholarã®æ¤œç´¢ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚: {e}")
            semsch_results = []

        all_papers = arxiv_results + semsch_results

        for paper in all_papers:
            paper_id = paper["url"].split("/")[-1]
            paper_record = {
                "id": paper_id,
                "title": paper["title"],
                "authors": paper["authors"],
                "year": paper["year"],
                "source": paper["source"],
                "query": keyword,
                "searched_at": datetime.now().isoformat(),
                "pdf_path": "",
                "text_path": "",
                "summary_path": "",
                "downloaded": 0,
                "summarized": 0,
                "url": paper.get("url", ""),
                "pdf_url": paper.get("pdf_url", ""),
                "background": "",   # ç©ºæ–‡å­—åˆ—ã§åˆæœŸåŒ–
                "purpose": "",
                "novelty": "",
                "method": "",
                "results": "",
                "discussion": "",
                "concerns": "",
                "conclusion": "",
                "future_work": "",
                "keywords": json.dumps([], ensure_ascii=False)  # ç©ºãƒªã‚¹ãƒˆã®JSONæ–‡å­—åˆ—
            }
            insert_or_update_paper(paper_record)

        if all_papers:
            st.write(f"ğŸ“„ PDFå–å¾—å¯èƒ½ãªè«–æ–‡æ•°: {len(all_papers)} ä»¶")

            for paper in all_papers:
                st.markdown(f"### [{paper['title']}]({paper['url']})")
                st.markdown(f"- è‘—è€…: {paper['authors']}")
                st.markdown(f"- å¹´: {paper['year']} / é›œèªŒ: {paper['venue']} / ã‚½ãƒ¼ã‚¹: {paper['source']}")
                st.markdown(f"- [ğŸ“„ PDFã‚’é–‹ã]({paper['pdf_url']})")
                st.markdown("---")

            df = pd.DataFrame(all_papers)
            st.dataframe(df[["source", "title", "authors", "year", "venue", "url", "pdf_url"]])

            csv = df.to_csv(index=False).encode("utf-8")
            st.download_button("ğŸ“… è«–æ–‡ä¸€è¦§ã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=csv, file_name=f"{keyword}_papers.csv", mime="text/csv")

            if st.button("ğŸ”½ æ¤œç´¢çµæœã®PDFã‚’ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼†è¦ç´„é–‹å§‹"):
                summaries = []

                for paper in all_papers:
                    try:
                        paper_id = paper["url"].split("/")[-1]
                        safe_title = paper['title'].replace(' ', '_').replace('/', '_')[:30]
                        filename = f"{paper['source']}_{paper['year']}_{safe_title}.pdf"

                        pdf_path = download_pdf(pdf_url=paper["pdf_url"], save_dir="data/pdf", filename=filename)

                        if pdf_path is None or not os.path.exists(pdf_path):
                            st.error(f"âŒ PDFã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ: {paper['title']}ã€‚")
                            continue

                        update_paper_status(paper_id, pdf_path=pdf_path, downloaded=1)

                        text = extract_text_from_pdf(pdf_path)

                        text_filename = f"{paper['source']}_{paper['year']}_{safe_title}.txt"
                        text_path = os.path.join("data/text", text_filename)
                        save_text_to_file(text, text_path)

                        update_paper_status(paper_id, text_path=text_path)

                        summary = summarize_text(text)
                        summary["ã‚¿ã‚¤ãƒˆãƒ«"] = paper["title"]
                        summary["ã‚½ãƒ¼ã‚¹"] = paper["source"]
                        summary["å¹´"] = paper["year"]
                        summaries.append(summary)

                        summary_path = save_summary_to_file(summary, pdf_path)
                        # sammaryã®è©³ç´°ã‚’DBã«ä¿å­˜
                        update_summary_to_db(paper_id, summary)
                        #sammaryãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã¨è¦ç´„æ¸ˆã¿ãƒ•ãƒ©ã‚°ã‚’æ›´æ–°
                        update_paper_status(paper_id, summary_path=summary_path, summarized=1)

                        st.success(f"âœ… {paper['title']} ã®è¦ç´„ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

                    except Exception as e:
                        st.error(f"âŒ {paper['title']} ã®å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

                if summaries:
                    df_summary = pd.DataFrame(summaries)
                    st.subheader("ğŸ“ è¦ç´„çµæœ")
                    st.dataframe(df_summary)

                    csv_summary = df_summary.to_csv(index=False).encode("utf-8")
                    st.download_button("ğŸ“… è¦ç´„çµæœã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=csv_summary, file_name="summaries.csv", mime="text/csv")


        else:
            st.warning("PDFä»˜ãã®è«–æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

# ---------------------------------------
# ğŸ“„ ã‚»ã‚¯ã‚·ãƒ§ãƒ³2ï¼šPDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦è¦ç´„
# ---------------------------------------
st.header("ğŸ§  PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦è¦ç´„")

uploaded_files = st.file_uploader("ğŸ“ PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆè¤‡æ•°å¯ï¼‰", type=["pdf"], accept_multiple_files=True)

if uploaded_files:
    summaries = []

    for uploaded_file in uploaded_files:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(uploaded_file.read())
            tmp_file_path = tmp_file.name

        with st.spinner(f"{uploaded_file.name} ã‚’è¦ç´„ä¸­..."):
            try:
                # ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
                text = extract_text_from_pdf(tmp_file_path)

                # ãƒ†ã‚­ã‚¹ãƒˆä¿å­˜
                safe_name = uploaded_file.name.replace(' ', '_').replace('/', '_')[:30]
                text_path = os.path.join("data/text", safe_name + ".txt")
                save_text_to_file(text, text_path)

                # è¦ç´„ç”Ÿæˆ
                summary = summarize_text(text)
                summary["ãƒ•ã‚¡ã‚¤ãƒ«å"] = uploaded_file.name
                summaries.append(summary)

                # è¦ç´„ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
                summary_path = save_summary_to_file(summary, tmp_file_path)

                # DBä¿å­˜å‡¦ç†ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãªã®ã§IDãŒãªã„å ´åˆã¯ãƒ•ã‚¡ã‚¤ãƒ«åã§ä»®IDï¼‰
                paper_id = safe_name  # ã‚‚ã—IDä½“ç³»ãŒã‚ã‚‹ãªã‚‰é©å®œå¤‰ãˆã¦ãã ã•ã„

                # ã¾ãšè«–æ–‡ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ç™»éŒ² or æ›´æ–°
                paper_record = {
                    "id": paper_id,
                    "title": uploaded_file.name,
                    "authors": "",
                    "year": "",
                    "source": "uploaded_pdf",
                    "query": "",
                    "searched_at": datetime.now().isoformat(),
                    "pdf_path": tmp_file_path,
                    "text_path": text_path,
                    "summary_path": summary_path,
                    "downloaded": 1,
                    "summarized": 1,
                    "url": "",
                    "pdf_url": "",
                    "background": summary.get("èƒŒæ™¯", ""),
                    "purpose": summary.get("ç›®çš„", ""),
                    "novelty": summary.get("æ–°è¦æ€§", ""),
                    "method": summary.get("æ–¹æ³•", ""),
                    "results": summary.get("çµæœ", ""),
                    "discussion": summary.get("è€ƒå¯Ÿ", ""),
                    "concerns": summary.get("æ‡¸å¿µç‚¹", ""),
                    "conclusion": summary.get("çµè«–", ""),
                    "future_work": summary.get("ä»Šå¾Œã®å±•æœ›", ""),
                    "keywords": json.dumps(summary.get("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", []), ensure_ascii=False)
                }
                insert_or_update_paper(paper_record)

            except Exception as e:
                st.error(f"{uploaded_file.name} ã®è¦ç´„ã«å¤±æ•—ã—ã¾ã—ãŸï¼š{e}")

    if summaries:
        df_summary = pd.DataFrame(summaries)
        st.subheader("ğŸ“ è¦ç´„çµæœ")
        st.dataframe(df_summary)

        csv_summary = df_summary.to_csv(index=False).encode("utf-8")
        st.download_button("ğŸ“… è¦ç´„çµæœã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=csv_summary, file_name="summaries.csv", mime="text/csv")


# ---------------------------------------
# ğŸ“„ ã‚»ã‚¯ã‚·ãƒ§ãƒ³3ï¼šä¿å­˜ãšã¿ã®è«–æ–‡ä¸€è¦§ã®è¡¨ç¤º
# ---------------------------------------

st.header("ğŸ“š ä¿å­˜æ¸ˆã¿è«–æ–‡ä¸€è¦§")

from utils.db_manager import fetch_all_papers

papers = fetch_all_papers()

if papers:
    import pandas as pd
    # ã„ãã¤ã‹ã®ä¸»è¦ã‚«ãƒ©ãƒ ã ã‘è¡¨ç¤ºä¾‹
    df = pd.DataFrame(papers)
    display_cols = [
        "id", "title", "authors", "year", "source", "downloaded", "summarized"
    ]
    st.dataframe(df[display_cols])

    # ã‚¯ãƒªãƒƒã‚¯ã§è©³ç´°è¡¨ç¤ºï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã‚¯ãƒªãƒƒã‚¯ãªã©ã®æ‹¡å¼µã‚‚å¯ï¼‰
    paper_id = st.text_input("è©³ç´°ã‚’è¦‹ãŸã„è«–æ–‡ã®IDã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

    if paper_id:
        selected = next((p for p in papers if p["id"] == paper_id), None)
        if selected:
            st.subheader(f"ğŸ“– {selected['title']}")
            st.write(f"è‘—è€…: {selected['authors']}")
            st.write(f"å¹´: {selected['year']}")
            st.write(f"ã‚½ãƒ¼ã‚¹: {selected['source']}")
            st.write(f"URL: {selected['url']}")
            st.write("---")
            st.markdown("### è¦ç´„å†…å®¹")
            st.write(f"**èƒŒæ™¯:** {selected.get('background', '')}")
            st.write(f"**ç›®çš„:** {selected.get('purpose', '')}")
            st.write(f"**æ–°è¦æ€§:** {selected.get('novelty', '')}")
            st.write(f"**æ–¹æ³•:** {selected.get('method', '')}")
            st.write(f"**çµæœ:** {selected.get('results', '')}")
            st.write(f"**è€ƒå¯Ÿ:** {selected.get('discussion', '')}")
            st.write(f"**æ‡¸å¿µç‚¹:** {selected.get('concerns', '')}")
            st.write(f"**çµè«–:** {selected.get('conclusion', '')}")
            st.write(f"**ä»Šå¾Œã®å±•æœ›:** {selected.get('future_work', '')}")
            st.write(f"**ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:** {selected.get('keywords', '')}")
        else:
            st.warning("æŒ‡å®šã•ã‚ŒãŸIDã®è«–æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
else:
    st.info("ã¾ã è«–æ–‡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

